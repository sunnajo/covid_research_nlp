{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import Medline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrez query pipeline\n",
    "def query(term):\n",
    "    # Create list of PMIDs\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=term, sort=\"pub+date\", retmax=500000)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    idlist = record[\"IdList\"]\n",
    "    return idlist\n",
    "\n",
    "def fetch(idlist):\n",
    "    # Fetch records using list of PMIDs\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\", retmode=\"text\", sort=\"pub+date\")\n",
    "    records = Medline.parse(handle)\n",
    "    \n",
    "    # Convert generator object to list\n",
    "    records = list(records)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_records_d(records):\n",
    "    '''\n",
    "    Takes in list of fetched records; iterates through each records and extracts specified information\n",
    "    into a dictionary, and appends dictionary to list; returns final list\n",
    "    '''\n",
    "    records_l = []\n",
    "    for record in records:\n",
    "        record_d = {}\n",
    "        record_d[\"pmid\"] = record.get(\"PMID\", \"?\")\n",
    "        record_d[\"title\"] = record.get(\"TI\", \"?\")\n",
    "        record_d[\"authors\"] = record.get(\"AU\", \"?\")\n",
    "        record_d[\"source\"] = record.get(\"SO\", \"?\")\n",
    "        record_d[\"location\"] = record.get(\"AD\", \"?\")\n",
    "        record_d[\"pub_date\"] = record.get(\"DP\", \"?\")\n",
    "        record_d[\"abstract\"] = record.get(\"AB\", \"?\")\n",
    "    records_l.append(record_d)\n",
    "    return records_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID-19 Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_idlist = query(\"covid-19\")\n",
    "len(covid_idlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_ids = covid_idlist.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query using slices of 200 ids at a time (limit)\n",
    "covid_records = []\n",
    "for i in range(0, 70703, 200):\n",
    "    one_slice = covid_ids[i:(i+200)]\n",
    "    covid_records.append(fetch(one_slice))\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn covid_records into flat list\n",
    "covid_records_l = [record for record_list in covid_records for record in record_list]\n",
    "len(covid_records_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each abstract, in addition to the abstract text, I would also like to extract:\n",
    "- PMID (unique identifier)\n",
    "- title\n",
    "- authors\n",
    "- source (journal)\n",
    "- location (institution(s))\n",
    "- published date\n",
    "- Entrez date (date entered into database)\n",
    "- country of journal\n",
    "- language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each abstract, compile list of dictionaries with desired variables\n",
    "covid_records_dicts = []\n",
    "for record in covid_records_l:\n",
    "    record_d = {}\n",
    "    record_d[\"pmid\"] = record.get(\"PMID\", \"?\")\n",
    "    record_d[\"title\"] = record.get(\"TI\", \"?\")\n",
    "    record_d[\"authors\"] = record.get(\"AU\", \"?\")\n",
    "    record_d[\"source\"] = record.get(\"SO\", \"?\")\n",
    "    record_d[\"location\"] = record.get(\"AD\", \"?\")\n",
    "    record_d[\"pub_date\"] = record.get(\"DP\", \"?\")\n",
    "    record_d[\"entrez_date\"] = record.get(\"EDAT\", \"?\")\n",
    "    record_d[\"country\"] = record.get(\"PL\", \"?\")\n",
    "    record_d[\"language\"] = record.get(\"LA\", \"?\")\n",
    "    record_d[\"abstract\"] = record.get(\"AB\", \"?\")\n",
    "    covid_records_dicts.append(record_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check length\n",
    "len(covid_records_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dictionaries into dataframe\n",
    "covid_records_df = pd.DataFrame(covid_records_dicts)\n",
    "covid_records_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for missing abstracts\n",
    "mask = (covid_records_df[\"abstract\"] == \"?\") | (covid_records_df[\"abstract\"] == \".\") | \\\n",
    "(covid_records_df[\"abstract\"] == \"Not available.\") | (covid_records_df[\"abstract\"] == \"Not required.\")\n",
    "missing_abstracts = covid_records_df[mask]\n",
    "missing_abstracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27,424 papers are missing abstracts. I will remove these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with no missing abstracts\n",
    "mask2 = (covid_records_df[\"abstract\"] != \"?\") & (covid_records_df[\"abstract\"] != \".\") & \\\n",
    "(covid_records_df[\"abstract\"] != \"Not available.\") & (covid_records_df[\"abstract\"] != \"Not required.\") & \\\n",
    "(covid_records_df[\"abstract\"] != \".\") & (covid_records_df[\"abstract\"] != \"NA.\")\n",
    "covid_abstracts_df = covid_records_df[mask2]\n",
    "covid_abstracts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df\n",
    "compression_opts = dict(method='zip', archive_name='covid_abstracts_df.csv')\n",
    "covid_abstracts_df.to_csv(r'/Users/sunnajo/Desktop/covid_abstracts.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that some papers were published prior to 2020. I primarily want to look at papers published after the first COVID-19 case was brought to light, which, per the WHO, was on December 31, 2019 in China.\n",
    "\n",
    "The 'pub_date' column looks inconsistent and there are some formatting issues. For accuracy's sake, I will use the 'entrez_date' column for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = covid_abstracts_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'entrez_date' column to datetime format\n",
    "df['entrez_date_dt'] = pd.to_datetime(df['entrez_date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year\n",
    "df['year'] = df['entrez_date_dt'].dt.year\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract month for papers published in 2020\n",
    "df_2020 = df[df['year'] == 2020]\n",
    "df_2020['month'] = df_2020['entrez_date_dt'].dt.month\n",
    "df_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at papers published in Jan & Feb to evaluate relevance\n",
    "df_2020[df_2020['month'] < 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are some relevant papers even in January and February."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn abstract column of dataframe into list: each item is an abstract\n",
    "abstracts_2020_l = list(df_2020[\"abstract\"])\n",
    "len(abstracts_2020_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at list of abstracts\n",
    "abstracts_2020_l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save list of abstracts text\n",
    "import pickle\n",
    "\n",
    "outfile = open('abstracts_2020_l.pkl','wb')\n",
    "pickle.dump(abstracts_2020_l,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from preprocessing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to create a British English to American English dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating British English to American English dictionary\n",
    "\n",
    "# Read in text files\n",
    "text_file = open(\"/Users/sunnajo/metis/onl20_ds4/british.txt\", \"r\")\n",
    "british_text = text_file.read()\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"/Users/sunnajo/metis/onl20_ds4/american.txt\", \"r\")\n",
    "american_text = text_file.read()\n",
    "text_file.close()\n",
    "\n",
    "# Convert each text file to list\n",
    "british_text = british_text.replace('\\n', ' ')\n",
    "british_text = british_text.split(' ')\n",
    "\n",
    "american_text = american_text.replace('\\n', ' ')\n",
    "american_text = american_text.split(' ')\n",
    "\n",
    "# Zip lists and create dictionary\n",
    "ab_list = list(zip(british_text, american_text))\n",
    "\n",
    "ab_dict = {}\n",
    "for pair in ab_list:\n",
    "    ab_dict[pair[0]] = pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Deleting words causing issues\n",
    "ab_dict\n",
    "\n",
    "del ab_dict['disc']\n",
    "del ab_dict['discs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic for preprocessing:\n",
    "- Remove content within parentheses: this content is often redundant (e.g. acronyms)\n",
    "- There are many compound terms adjoined with '-'. Since each word may have meaning, I will separate these words and try to capture them by grouping my corpus into bigrams and trigrams.\n",
    "- I noticed that some words are in British English. I will convert these terms to American English so as to maintain consistency and optimize the quality of my preprocessing.\n",
    "- There are terms that occur frequently in my corpus that are redundant in light of the scope of this project and that do not add value in interpretation. I will add these to my list of stopwords in addition to the standard English stopwords in NLTK.\n",
    "- I will look only at nouns as I have many terms and these will likely be the most valuable for topic modeling\n",
    "- I will remove words <4 letters in length as these are likely to have little semantic value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text using pipeline\n",
    "processed_text = pp_pipeline(abstracts_2020_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check length of preprocessed text list\n",
    "len(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed text\n",
    "outfile = open('processed_text.pkl','wb')\n",
    "pickle.dump(processed_text,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find abstracts with 0 terms after pre-processing\n",
    "tokens = [word_tokenize(text) for text in processed_text]\n",
    "\n",
    "zero_terms = []\n",
    "for idx, text in enumerate(tokens):\n",
    "    if len(text) == 0:\n",
    "        zero_terms.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Abstracts with 0 terms after pre-processing - n_gram threshold 300\n",
    "zero_terms = [10024,\n",
    " 12295,\n",
    " 12375,\n",
    " 13667,\n",
    " 24863,\n",
    " 31641,\n",
    " 31658,\n",
    " 31659,\n",
    " 33122,\n",
    " 33173,\n",
    " 34006,\n",
    " 36422,\n",
    " 36427,\n",
    " 39538,\n",
    " 39544,\n",
    " 39741,\n",
    " 41600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_terms.reverse()\n",
    "\n",
    "for i in zero_terms:\n",
    "    del processed_text[i]\n",
    "\n",
    "len(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing abstracts from df\n",
    "for i in zero_terms:\n",
    "    df_2020.drop(df_2020.index[i], inplace=True)\n",
    "\n",
    "df_2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of PMIDs to use as abstract labels\n",
    "pmids = list(df_2020['pmid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed text - bigram/trigram, 2020 only\n",
    "outfile = open('pmids.pkl','wb')\n",
    "pickle.dump(pmids,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save altered df\n",
    "compression_opts = dict(method='zip', archive_name='covid_abstracts_2020.csv')\n",
    "df_2020.to_csv(r'/Users/sunnajo/Desktop/covid_abstracts_2020.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
